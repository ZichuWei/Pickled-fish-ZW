---
title: "Assignment2"
author: "Zichu Wei u7457435"
date: "2022/10/29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Provide the link about my GitHub Repo 

[My GitHub Repository](https://github.com/ZichuWei/Pickled-fish-ZW)


## Statistical Analysis and Interpretation

### Load the necessary R Packages:

```{r,results='hide'}
# Data processing:
library(tidyverse)
library(readr)
library(dplyr)
library(plotrix)
```

```{r}
# Meta_analysis:
library(pacman)
pacman::p_load(metafor, orchaRd)
p_load(bookdown, devtools, tidyverse, ggforce, GGally, flextable, latex2exp, png,
    magick, metafor, MASS, emmeans, R.rsp)
devtools::install_github("daniel1noble/orchaRd", force = TRUE)
library(orchaRd)
```

```{r}
# Draw the figures:
library(ggplot2)
```


### Read the files:

```{r}
# These analyses need these three files.
data <- read.csv("OA_activitydat_20190302_BIOL3207.csv")
paper <- read.csv("clark_paper_data.csv")
meta <- read.csv("ocean_meta_data.csv")
```

### Process and clean the original data:

```{r}
# Drop irrelevant columns 'loc' and 'comment':
data_clean <- subset(data,select = -c(loc, comment))
# Remove the missing data, which reflected by 'NA';
# Remove the incorrect data: some data cannot less than 0, so use filter to check and correct the data.
data_eff <- data_clean %>% filter(!is.na(activity)) %>% filter(animal_id > 0, activity > 0)
data_eff
```

```{r}
# Check spelling in species and treatment:
data_eff$species %>% unique()
data_eff$treatment %>% unique()
```

```{r}
# Because we find that there are six species and two treatments which are all correct, so we don't need to change or remove some data.
```


### 1. Summary statistics:

```{r}
# Because the following analyses need to analyze different treatment, so use 'filter' to separate the treatment:
data_ctrl <- data_eff %>% filter(treatment=="control")
data_oa <- data_eff %>% filter(treatment=="CO2")
```

#### Gnerate the summary staistics for different levels:

Use 'group_by' and 'summarize' to generate the summary statistics (means, SD, N) for each fish species' average activity for each treatment.

```{r}
# Control: According to the column name in the 'meta-data_ocean_meta.csv' file, set the column as 'ctrl.mean', 'ctrl.sd', 'ctrl.n':
data1 <- data_ctrl %>% group_by(species) %>% summarize(ctrl.mean = mean(activity), ctrl.sd = sd(activity), ctrl.n = length(species))
data1
```

```{r}
# CO2: According to the column name in the 'meta-data_ocean_meta.csv' file, set the column as 'oa.mean', 'oa.sd', 'oa.n':
data2 <- data_oa %>% group_by(species) %>% summarize(oa.mean = mean(activity), oa.sd = sd(activity), oa.n = length(species))
data2
```

```{r}
# Merge the summary statistics for the following processing:
data_1 <- merge(data1,data2)
data_1
```


### 2-3. Merge summary statistics with the metadata:

#### 2. Metadata in 'clark_paper_data.csv' file:

Merge the summary statistics generated from the above with the metadata in 'clark_paper_data.csv' file:

```{r}
# Use 'merge' function to merge 'data_1' with 'paper', let the sequence as 'paper' in the left.
data_2 <- merge(paper,data_1)
data_2
```

#### 3. Metadata in 'ocean_meta_data.csv' file:

Merge the combined summary statistics 'data_1' and metadata from Clark et al. 'paper' (output 'data_2') into the larger meta-analysis dataset 'ocean_meta_data.csv' file:

```{r}
# Use 'rbind()' function to merge 'data_2' with 'meta':
# We can find that 'meta' and 'data_2' have different colnames, and both have very informal colnames, so we can use 'janitor::clean_names()' to polish them:
data_3 <- rbind(meta %>% janitor::clean_names(),data_2 %>% janitor::clean_names())
data_3$residual <- 1:dim(data_3)[1]
# We find 'oa_mean' and 'oa_sd' both have some NA values, in order to help the following analysis, then we first remove them.
data_4 <- data_3 %>% filter(!is.na(oa_mean)) %>% filter(!is.na(oa_sd))
summary(data_4)
```



### 4. Calculate lnRR effect size:

Correctly calculate the log response ratio (lnRR) effect size for every row of the dataframe using metafor’s 'escalc()' function

```{r}
# Avoid NA values, we choose the value greater than 0:
data_complete <- data_4 %>% filter(ctrl_mean>0,ctrl_n>0,ctrl_sd>0,oa_n>0,oa_mean>0,oa_sd>0)
data_lnRR <- escalc(measure="ROM", m1i=ctrl_mean, m2i=oa_mean, sd1i=ctrl_sd, sd2i=oa_sd, n1i=ctrl_n, n2i=oa_n,data=data_complete)
str(data_lnRR)
# 'yi' means the value of 'lnRR' effect size.
```


### 5. Controls for the sampling variance of lnRR:

Correct meta-analytic model fitted to the data that controls for the sampling variance of lnRR. The model should include a random effect of study and observation. Use metafor’s 'rma.mv()' function.

```{r}
# Random effect analysis:
data_v1 <- rma.mv(yi, vi, test = "t", dfs = "contain", data = data_lnRR)
summary(data_v1)
```

```{r}
# Multiple-level meta-analytic model:
# Estimate a random effect variance for between study (i.e., study) and within-study (i.e., residual) grouping variables to control for non-independence and understand sources driving effect size variability.
data_v <- metafor::rma.mv(yi, vi, 
                   method="REML",
                   random=list(~1|study,
                               ~1|residual), 
                   dfs = "contain",
                   test="t",
                   data=data_lnRR)
summary(data_v)
```

```{r}
# The explanation of the above code is below.
```


### 6. Written paragraph:

Written paragraph of the findings and what they mean which is supported with a figure. The paragraph should include:

#### 1) Overall meta-analytic mean & Uncertainty in the overall meta-analytic mean:

Correct presentation and interpretation of overall meta-analytic mean and measures of uncertainty around the mean estimate (e.g., 95% confidence intervals).

```{r}
# Overall meta-analytic mean:
# From the model by extracting the intercept (labeled ‘estimate’ in the model output), we can know what the overall meta-analytic mean effect size across the studies actually is estimated to be. And the model is just an object that stores all the values for us.
# We can extract the estimate using the 'coef' function, it is estimated to be -0.1497, which tells us that the mean 'yi' value is negative, and there is a rather weak overall association between physiology and dispersal / movement when we pool across all studies.
```

```{r}
# Use transformation 'rma': convert the overall meta-analytic mean back to the correlation coefficient.
predict(data_v, transf = "transf.rma")
```

```{r}
# Uncertainty in the overall meta-analytic mean:
# 95% confidence intervals are important to provide and they are stored in the data_v object as ci.lb and ci.up. We can extract the 95% confidence intervals in the table which range from -0.3810 to 0.0817, that is, 95% of the time we would expect the true mean to fall between 'yi' values of -0.3810 to 0.0817. And if we were to repeat the experiment many times, 95% of the confidence intervals constructed would contain the true meta-analytic mean.
```
 
```{r} 
# Testing the hypothesis about whether the overall meta-analytic mean is different from 0.
# We can also see that the null hypothesis that yi = 0 can be rejected because there is a significantly smaller estimate than a correlation of 0, which we can see from the p-value being < 0.05. To be more exact, the p-value is < 0.0001.
```


#### 2) Measures of heterogeneity in effect size estimates:

Measures of heterogeneity in effect size estimates across studies (i.e., I2 and/or prediction intervals - see 'predict()' function in 'metafor')

```{r, het, tab.cap = "Total effect size hetereogneity (Total), as well as the proportion of hetereogeneity in effects resulting from Study and Residual / Observational" }
# I2 measurement:
# Calculate I2:
i2_vals <- orchaRd::i2_ml(data_v)

# Clean up the names of the different I2 estimates and use some regular expressions to fix that. 'gsub()' function is used for replacement operations, change the original 'I2_' to blank. And use 'firstup()' to make the first letter of what is left 'type' capitalised.
i2 <- tibble(type = firstup(gsub("I2_", "", names(i2_vals))), I2 = i2_vals)

# Use 'flextable' to make a pretty table.
flextable(i2) %>%
    align(part = "header", align = "center") %>%
    compose(part = "header", j = 1, value = as_paragraph(as_b("Type"))) %>%
    compose(part = "header", j = 2, value = as_paragraph(as_b("I"), as_b(as_sup("2")),
        as_b("(%)"))) %>% autofit(add_h = 0.5, part = c("body", "header"))
```

```{r}
# According to the I2 of total, we can conclude that we have highly heterogeneous effect size data because sampling variation.
# From the multi-level meta-analytic model we find that only 10% of the total variation in effect size estimates is the result of differences between studies.
```


```{r}
# Prediction intervals measurement:
predict(data_v)
```

```{r}
# The prediction intervals are labelled 'pi.lb' (lower bound) and 'pi.ub' (upper bound), in this model, the prediction intervals are range from -4.4483 to 4.1490. It means 95% prediction intervals are wide. Effect sizes (yi) are expected to range from -4.4483 to 4.1490 95% of the time with repeated experiments, suggesting a lot of inconsistency between studies.
```


#### 3) Forest plot:

Forest plot showing the mean estimate, 95% confidence interval, and prediction interval with clearly labelled axes, number of samples and studies plotted on figure.

```{r rochard, fig.cap= "Orchard plot showing the correlation coefficients estimated in the intrcpt. k = the number of effect sizes and the number of studies are in brackets. The size of the effect is scaled by the precision of each effect size value, which is 1 / sqrt(vir)"}
# Make an orchard plot using the model object, and the orchard plot improved from the forest plot.
orchaRd::orchard_plot(data_v, mod = "1", group = "study", data = data_lnRR,
    xlab = "Correlation Coefficient", angle = 45)
```

```{r}
# The orchard plot as a variant on the classic forest plot, cultivated to the needs of meta‐analysts in ecology and evolution, showing the mean for correlation coefficients estimated between physiology and activity, dispersal and behavior. k = the number of effect sizes 757 (the number of studies = 91). The size of the effect is scaled by the precision of each effect size value, there are more numbers in the middle distribution, which is less accurate.
```


### 7. Funnel plot:

Funnel plot for visually assessing the possibility of publication bias.

#### 'funnel' method:

```{r}
# Use 'fuunel()' function to draw funnel plot:
metafor::funnel(x = data_lnRR$yi, vi = data_lnRR$vi,yaxis = "seinv",
    digits = 2, level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray 75"),
    las = 1, xlab = "Correlation Coefficient (r)", atransf = tanh, legend = TRUE)
# Because we find there are some very high values, so we need to remove them:
# Because the we need the data about '1/sqrt(vi)',so there is a negative correlation, so we need to remove some lowest values.
# First method: Try to remove the values to see if you can get a graph that is easy to observe.
data_lnRRr <- subset(data_lnRR, vi!=min(data_lnRR$vi))
data_lnRRc <- subset(data_lnRRr, vi!=min(data_lnRRr$vi))
data_lnRRd <- subset(data_lnRRc, vi!=min(data_lnRRc$vi))
data_lnRRe <- subset(data_lnRRd, vi!=min(data_lnRRd$vi))
data_lnRRf <- subset(data_lnRRe, vi!=min(data_lnRRe$vi))
```

```{r  funnel, echo=TRUE, fig.align='center', fig.cap= "Funnel plot depicting the correlation between metabolism and fitness as a function of precision (1 / SE). The dotted lines are the theoretical 95% sampling variance intervals - the interval with which we expect effect size estimates to fall within if only sampling variance drives differences in effects. Shaded regions represent the p-value of studies. The white region indicates studies where the p-value is between 0.1 and 1; dark gray where the p-value of studies is between 0.05 and 0.1 and the lighter gray regions where the p-value of studies is significant." }
# Check and recreate the figure:
metafor::funnel(x = data_lnRRf$yi, vi = data_lnRRf$vi, yaxis = "seinv",
    digits = 3, level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray 75"),
    xlab = "Correlation Coefficient (r)", xlim=c(-5,5),atransf = tanh, legend = TRUE, main = "Funnel plot")

# Narrow down the X-axis, enlarge the image to help you see it:
metafor::funnel(x = data_lnRRf$yi, vi = data_lnRRf$vi, yaxis = "seinv",
    digits = 3, level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray 75"), col="orange",
    xlab = "Correlation Coefficient (r)", xlim=c(-1,1), atransf = tanh, legend = TRUE, main = "Funnel plot")
```

```{r  funnel, echo=TRUE, fig.align='center', fig.cap= "Funnel plot depicting the correlation between metabolism and fitness as a function of precision (1 / SE). The dotted lines are the theoretical 95% sampling variance intervals - the interval with which we expect effect size estimates to fall within if only sampling variance drives differences in effects. Shaded regions represent the p-value of studies. The white region indicates studies where the p-value is between 0.1 and 1; dark gray where the p-value of studies is between 0.05 and 0.1 and the lighter gray regions where the p-value of studies is significant." }
# Second method: directly give the range of x-axis and y-axis:
metafor::funnel(x = data_lnRRf$yi, vi = data_lnRRf$vi, yaxis = "seinv",
    digits = 3, level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray 75"),
    xlab = "Correlation Coefficient (r)", xlim=c(-5,5), ylim=c(1,150),atransf = tanh, legend = TRUE, main = "Funnel plot")
```

#### 'ggplot' method:

```{r, ggplotfunnel, fig.align='center', fig.cap="Funnel plot showing the precision of effects against their correlation"}
# Use 'ggplot' to draw a simple funnel plot:
ggplot(data_lnRRf, aes(y = 1/sqrt(vi), x = tanh(yi))) + geom_point() + geom_vline(aes(xintercept = 0)) + labs(y = "Precision (1/SE)", x = "Correlation Coefficient (r)") + theme_bw() +ggtitle("Funnel plot showing the precision of effects against correlation") +theme(plot.title = element_text(hjust = 0.5))
```

```{r}
# Explanation of the funnel plot: There is a noticeable blank space in the bottom right corner with positive correlations based on very small sample sizes that are generally moderate to small in magnitude going unpublished. The contour-enhanced funnel plot also tells us that these are studies that failed to find a significant correlation due to the small amount of data on both ends. And if the magnitude of correlation is large enough in the positive direction even with small sample sizes these can get published, but for the most part these are significant at 0.05.
# Expect under a file-drawer situation (i.e., where researchers stash away poorer quality studies showing opposite effects in their desk drawers) that studies with low power (i.e., low precision, wide standard errors, and small sample sizes) and non-significant correlations will go unpublished. This should be particularly true for studies that show the opposite: positive correlations from studies with small sample sizes / high precision that are not significant. This is one factor that can drive what we call funnel asymmetry, showing a bunch of missing effect sizes in the bottom right corner of the funnel.
```


#### Fitting a Multilevel Meta-Regression model to Test and Correct for Publication bias:

Predict if we fit a meta-regression model that uses sampling variance as a fixed effect / moderator, there will be a significant slope coefficient. That is because there is an unequal distribution of effect sizes on either side of the funnel plot and the mean effect size when the sampling variance is high gets shifted resulting in the slope being different from 0.

```{r, egger, fig.align='center',fig.cap= "Plot of lnRR against sampling variance for Zr. A linear model was fit to the data."}
# Use 'ggplot' to fit a multilevel meta-regression model:
ggplot(data_lnRR, aes(y = yi, x = vi)) + 
  geom_point() + 
  geom_smooth(method = lm) + 
  labs(y = "Correlation Coefficient", x = "Sampling Variance")+
  theme_classic()+ggtitle("Sampling variance plot") +theme(plot.title = element_text(hjust = 0.5))
# Because there are some very high values, so we can limit the range of x-axis.
# Create the correct plot:
ggplot(data_lnRR, aes(y = yi, x = vi)) + 
  geom_point() + 
  geom_smooth(method = lm) + 
  labs(y = "Correlation Coefficient", x = "Sampling Variance")+ xlim(0,20)+
  theme_classic()+ggtitle("Sampling variance plot") +theme(plot.title = element_text(hjust = 0.5))
```

```{r}
# Including sampling variance as moderator
metareg_v <- rma.mv(yi ~ vi, vi, 
                    random = list(~1|study, 
                                  ~1|residual), 
                    test = "t", dfs = "contain", 
                    data = data_lnRR)
summary(metareg_v)
```

```{r}
# Explore how much variation about this model:
r2 <- orchaRd::r2_ml(metareg_v)
r2
```

```{r}
# Sampling variance explains 2.8% of effect size variance. This is the marginal R2, which tell us how much variation the ‘fixed effects’ or moderators explain in the model. Conditional R2 tells us that the full model, that accounts for the both the fixed and random effects, explains 12.8% of variance in effect size.
# There is evidence for publication bias because the slope estimate for vi is significant. We can see from this model that the adjusted lnRR (yi) when there is no uncertainty (i.e., the intercept) is 0.05 with a 95% confidence interval that overlaps zero (i.e., 95% CI = -0.3810 to 0.0817), that is, if no uncertainty around estimates exists, or we have a very high powered set of studies than we would expect the correlation to be, on average, 0.05.
```


### 8. Time-lag plot:

Time-lag plot assessing how effect sizes may or may not have changed through time.

```{r yearbubble,fig.align='center',fig.cap="Plot of r as a function of publication year (print). Points are scaled in relation to their precision (1/sqrt(vi). Small points indicate effects with low precision or high sampling varaince"}
# Because there are two year datasets, actually the results are similar:
# Year_print:
ggplot(data_lnRR, aes(y = yi, x = year_print, size = 1/sqrt(vi))) + geom_point(alpha = 0.2) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year",
    y = "Correlation Coefficien(r)", size = "Precision (1/SE)") +
    theme_classic()+ggtitle("Time-lag plot with publication year (print)") +theme(plot.title = element_text(hjust = 0.5))
```

```{r yearbubble,fig.align='center',fig.cap="Plot of r as a function of publication year (online). Points are scaled in relation to their precision (1/sqrt(vi). Small points indicate effects with low precision or high sampling varaince"}
# Year_online:
ggplot(data_lnRR, aes(y = yi, x = year_online, size = 1/sqrt(vi))) + geom_point(alpha = 0.2) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year",
    y = "Correlation Coefficien(r)", size = "Precision (1/SE)") +
    theme_classic()+ggtitle("Time-lag plot with publication year (online)") +theme(plot.title = element_text(hjust = 0.5))
```

```{r}
# Explanation of these two Time-lag plots:
# 1) There does appear to be a positive relationship with year.
# 2) The earlier year studies have lower sampling variance (i.e., high precision), but actually the overall difference is small.
# 3) These early studies appear to have a lower effect size compared with studies that are done in later years.
```



